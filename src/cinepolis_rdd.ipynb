{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224f0b80",
   "metadata": {},
   "source": [
    "# 1. Email Uplift Analysis: Regression Discontinuity + ML\n",
    "\n",
    "Goal:\n",
    "- Measure whether marketing emails increase near-term movie ticket purchases.\n",
    "\n",
    "Approach:\n",
    "- Build an around-the-day (ADT) panel for +/- K days around each email to enable an RDD-style before/after contrast.\n",
    "- Train an ML model (CatBoost) to capture non-linearities and interactions, and assess the post-email effect.\n",
    "- Fit a GLM with cluster-robust SE at customer level to estimate the post-email discontinuity.\n",
    "\n",
    "Deliverables:\n",
    "- Clear uplift estimate with uncertainty.\n",
    "- Feature insights from ML (SHAP) to understand drivers.\n",
    "- Actionable recommendations and caveats for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ab74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports: core DS stack and modeling libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import uuid\n",
    "\n",
    "# Stats and modeling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import holidays\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import shap\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f4f17",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data\n",
    "\n",
    "We load transactions, pricebook, customers, emails, and releases. Dates are parsed and basic integrity filters are applied.\n",
    "\n",
    "Insights (summary):\n",
    "- We keep only valid transactions with positive tickets and revenue.\n",
    "- Email timestamps are normalized to date granularity for daily paneling.\n",
    "- Releases are filtered to those with positive sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data CSVs from the repo data/ folder\n",
    "# Note: paths are relative to this notebook location (src/)\n",
    "df_transactions = pd.read_csv(\"../data/Transactions.csv\", header=0)\n",
    "df_pricebook = pd.read_csv(\"../data/Pricebook.csv\", header=0)\n",
    "df_customers = pd.read_csv(\"../data/Customers.csv\", header=0)\n",
    "df_emails = pd.read_csv(\"../data/Emails.csv\", header=0)\n",
    "df_releases = pd.read_csv(\"../data/Releases.csv\", header=0, sep=\"|\")\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"Transactions:\", df_transactions.shape)\n",
    "print(\"Pricebook:\", df_pricebook.shape)\n",
    "print(\"Customers:\", df_customers.shape)\n",
    "print(\"Emails:\", df_emails.shape)\n",
    "print(\"Releases:\", df_releases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions wrangling\n",
    "# - Create a transaction_id per (member, transaction date, show date)\n",
    "# - Keep only positive tickets and revenue\n",
    "# - Aggregate duplicates per ID; parse dates\n",
    "\n",
    "df_transactions['transaction_id'] = df_transactions.apply(\n",
    "    lambda x: str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{x['CARD_MEMBERSHIPID']}_{x['FECHA_TRANSACCION']}_{x['FECHA_FUNCION']}\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter for valid sales\n",
    "df_transactions = df_transactions.query('BOLETOS > 0').reset_index(drop=True)\n",
    "df_transactions = df_transactions.query('IMPORTE_TAQUILLA > 0').reset_index(drop=True)\n",
    "\n",
    "# Aggregate per transaction_id\n",
    "cols_agg = {\n",
    "    'ID_CINE': 'first',\n",
    "    'FECHA_TRANSACCION': 'first',\n",
    "    'FECHA_FUNCION': 'first',\n",
    "    'CARD_MEMBERSHIPID': 'first',\n",
    "    'ID_MARCA': 'first',\n",
    "    'TX_PELICULA_UNICA': 'first',\n",
    "    'BOLETOS': 'sum',\n",
    "    'IMPORTE_TAQUILLA': 'sum'\n",
    "}\n",
    "\n",
    "df_transactions = (\n",
    "    df_transactions\n",
    "    .groupby('transaction_id', as_index=False)\n",
    "    .agg(cols_agg)\n",
    "    .assign(\n",
    "        FECHA_TRANSACCION=lambda x: pd.to_datetime(x['FECHA_TRANSACCION'], format='%Y-%m-%d', errors='coerce'),\n",
    "        FECHA_FUNCION=lambda x: pd.to_datetime(x['FECHA_FUNCION'], format='%Y-%m-%d', errors='coerce')\n",
    "    )\n",
    "    .sort_values('FECHA_TRANSACCION', ascending=True, ignore_index=True)\n",
    ")\n",
    "\n",
    "print(\"Transactions after cleaning:\", df_transactions.shape)\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join emails with customers for a consistent subscriber set\n",
    "# - Normalize timestamps to dates and extract hour for potential use\n",
    "\n",
    "df_emails = (\n",
    "    df_emails\n",
    "    .merge(df_customers, how='inner', left_on=\"SubscriberKeyH\", right_on=\"SubscriberKey\")\n",
    "    .drop(columns=[\"SubscriberKeyH\", \"SubscriberKey\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Parse timestamp and derive date and hour\n",
    "df_emails['EventDate_TZ'] = pd.to_datetime(df_emails['EventDate'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "df_emails['EventDate'] = df_emails['EventDate_TZ'].dt.date\n",
    "\n",
    "df_emails['EventDate_Hour'] = df_emails['EventDate_TZ'].dt.hour\n",
    "\n",
    "print(\"Emails post-merge:\", df_emails.shape)\n",
    "\n",
    "df_releases['ESTRENO'] = pd.to_datetime(df_releases['ESTRENO'], format='%Y-%m-%d', errors='coerce')\n",
    "df_releases = df_releases.query('VENTAS > 0').reset_index(drop=True)\n",
    "\n",
    "print(\"Releases after basic cleaning:\", df_releases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a holiday calendar for Mexico over the transactions coverage\n",
    "start_date = df_transactions['FECHA_TRANSACCION'].min()\n",
    "end_date = df_transactions['FECHA_TRANSACCION'].max()\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "mx_holidays = holidays.MX(years=date_range.year.unique())\n",
    "\n",
    "holiday_dates = [d for d in date_range if d in mx_holidays]\n",
    "holiday_names = [mx_holidays[d] for d in holiday_dates]\n",
    "df_mx_holidays = pd.DataFrame({'date': holiday_dates, 'holiday': holiday_names})\n",
    "\n",
    "df_mx_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ca5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-movie windows and a blockbuster intensity index over time\n",
    "# - For each movie: from 2 days before release to last show in transactions\n",
    "# - Blockbuster score: log2 of sales (VENTAS); also a smoothed time series\n",
    "\n",
    "df_releases_range = (\n",
    "    df_transactions\n",
    "    .groupby('TX_PELICULA_UNICA', as_index=False)\n",
    "    .agg(\n",
    "        date_end=('FECHA_FUNCION', 'max'),\n",
    "        tickets_sold=('BOLETOS', 'sum'),\n",
    "        money_sold=('IMPORTE_TAQUILLA', 'sum'),\n",
    "    )\n",
    "    .merge(df_releases, how='inner', on='TX_PELICULA_UNICA')\n",
    "    .assign(\n",
    "        days_on_air=lambda x: (x['date_end'] - x['ESTRENO']).dt.days,\n",
    "        blockbuster_score=lambda x: np.log2(x['VENTAS']),\n",
    "        date_before_release=lambda x: x['ESTRENO'] - pd.Timedelta(days=2),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build daily series of concurrent movies and total blockbuster score\n",
    "date_range = pd.date_range(start=df_releases_range['date_before_release'].min(),\n",
    "                           end=df_releases_range['date_end'].max())\n",
    "\n",
    "ts_movies_on_air = (\n",
    "    pd.DataFrame({'date': date_range})\n",
    "    .assign(\n",
    "        on_air=lambda x: x['date'].apply(\n",
    "            lambda d: df_releases_range.query('date_before_release.le(@d) and date_end.ge(@d)').shape[0]\n",
    "        ),\n",
    "        blockbuster_score=lambda x: x['date'].apply(\n",
    "            lambda d: df_releases_range.query('date_before_release.le(@d) and date_end.ge(@d)')['blockbuster_score'].sum()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# look\n",
    "ts_movies_on_air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer child presence from ticket unit price vs. reference price\n",
    "# - Join pricebook by cinema; if unit price < reference, mark as child ticket\n",
    "# - Build per-customer flags and aggregates (has_children)\n",
    "\n",
    "df_transactions = df_transactions.merge(df_pricebook, how='left', on='ID_CINE')\n",
    "\n",
    "df_transactions = (\n",
    "    df_transactions\n",
    "    .assign(\n",
    "        had_children_ticket=lambda x: np.where(\n",
    "            (x['IMPORTE_TAQUILLA'] / x['BOLETOS']).lt(x['PRECIO_POL_R']), 1, 0\n",
    "        ),\n",
    "        has_children=lambda x: x.groupby('CARD_MEMBERSHIPID')['had_children_ticket'].transform(lambda y: y.any().astype(int))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Child-ticket transactions:\", int(df_transactions['had_children_ticket'].sum()))\n",
    "print(\"% child-ticket transactions:\", df_transactions['had_children_ticket'].mean() * 100)\n",
    "\n",
    "# Build time-series aggregates per customer (visits, spend)\n",
    "df_transactions = (\n",
    "    df_transactions\n",
    "    .sort_values(by=['CARD_MEMBERSHIPID', 'FECHA_TRANSACCION'], ignore_index=True)\n",
    "    .assign(\n",
    "        cum_visits=lambda x: x.groupby('CARD_MEMBERSHIPID')['transaction_id'].cumcount() + 1,\n",
    "        cum_spend=lambda x: x.groupby('CARD_MEMBERSHIPID')['IMPORTE_TAQUILLA'].cumsum(),\n",
    "        avg_spend_per_visit=lambda x: x.groupby('CARD_MEMBERSHIPID')['IMPORTE_TAQUILLA'].transform('mean'),\n",
    "        total_visits=lambda x: x.groupby('CARD_MEMBERSHIPID')['transaction_id'].transform('count'),\n",
    "    )\n",
    "    .sort_values(['total_visits', 'CARD_MEMBERSHIPID', 'FECHA_TRANSACCION'], ascending=[False, True, True], ignore_index=True)\n",
    ")\n",
    "\n",
    "# Favorite cinema per customer (mode of historical visits)\n",
    "df_transactions['favorite_cinema'] = (\n",
    "    df_transactions\n",
    "    .groupby('CARD_MEMBERSHIPID')['ID_CINE']\n",
    "    .transform(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    ")\n",
    "\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfa514",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ADT Panel Construction (RDD-ready)\n",
    "\n",
    "We build a daily panel around each email, +/- K days. This creates matched pre/post windows to estimate the email’s near-term effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6cffa",
   "metadata": {},
   "source": [
    "### 3.1: Configure window and filter emails\n",
    "- We subset emails to a recent time range and convert event dates to pandas datetime.\n",
    "- The preview above shows the first rows (member, send, date) we will expand into a daily panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Configure window and filter emails\n",
    "K = 7\n",
    "print(f\"Window size K: {K} days\")\n",
    "\n",
    "# Filter a recent email window and parse dates\n",
    "_df = (\n",
    "    df_emails\n",
    "    .loc[lambda x: (x['EventDate'] >= pd.to_datetime('2024-01-01').date()) &\n",
    "                  (x['EventDate'] <= pd.to_datetime('2024-07-15').date())]\n",
    "    .assign(EventDate=lambda x: pd.to_datetime(x['EventDate'], format='%Y-%m-%d', errors='coerce'))\n",
    ")\n",
    "print(\"Emails in window:\", _df.shape)\n",
    "\n",
    "# Preview the filtered emails (first 5)\n",
    "_df[['CARD_MEMBERSHIPID', 'SendId', 'EventDate']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fce93",
   "metadata": {},
   "source": [
    "#### 3.2: Expand emails into daily windows\n",
    "- For each email date, we create a list of dates from K days before to K days after.\n",
    "- The preview above shows the list we will explode into one row per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Step 2: Build +/-K day ranges per email\n",
    "_df_ranges = _df.assign(\n",
    "    dates_to_expand=lambda x: x['EventDate'].apply(lambda d: pd.date_range(d - pd.Timedelta(days=K), d + pd.Timedelta(days=K), freq='D'))\n",
    ")\n",
    "print(\"Ranges built:\", _df_ranges.shape)\n",
    "\n",
    "# Preview first few range lists\n",
    "_df_ranges[['CARD_MEMBERSHIPID', 'SendId', 'EventDate', 'dates_to_expand']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44928e",
   "metadata": {},
   "source": [
    "#### 3.3: Explode to daily panel\n",
    "- Exploding the date lists gives us a daily panel per email and customer.\n",
    "- The head above shows how each email now maps to multiple dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf937cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Step 3: Expand to one row per day per email\n",
    "emails_daily = (\n",
    "    _df_ranges\n",
    "    .loc[:, ['CARD_MEMBERSHIPID', 'SendId', 'dates_to_expand']]\n",
    "    .explode('dates_to_expand')\n",
    "    .rename(columns={'dates_to_expand': 'EventDate'})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"Daily rows:\", emails_daily.shape)\n",
    "\n",
    "# Preview first daily rows\n",
    "emails_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82b5aa",
   "metadata": {},
   "source": [
    "#### 3.4: Add relative day index\n",
    "- We assign a relative day index centered at 0 on the send day; negative is pre, positive is post.\n",
    "- The head above shows the panel now includes the relative position around each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bcbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Step 4: Add relative index from email day and keep [-K, K]\n",
    "emails_daily['days_from_email'] = emails_daily.groupby(['CARD_MEMBERSHIPID', 'SendId']).cumcount() - K\n",
    "print(\"days_from_email min/max:\", int(emails_daily['days_from_email'].min()), int(emails_daily['days_from_email'].max()))\n",
    "\n",
    "emails_daily = emails_daily.query(\"days_from_email.between(-@K, @K)\").reset_index(drop=True)\n",
    "print(\"Daily panel after trimming:\", emails_daily.shape)\n",
    "\n",
    "# Preview daily panel with relative day\n",
    "emails_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c60ced",
   "metadata": {},
   "source": [
    "#### 3.5: Add transactions to panel\n",
    "- We add daily context features to help control for demand: blockbuster intensity, holidays, releases, and weekly/monthly cycles.\n",
    "- The head above previews these columns alongside the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Step 5: Enrich with context (blockbuster, calendar, holidays)\n",
    "emails_daily_ctx = (\n",
    "    emails_daily\n",
    "    .merge(ts_movies_on_air[['date', 'blockbuster_score']], how='left', left_on='EventDate', right_on='date')\n",
    "    .drop(columns=['date'])\n",
    "    .assign(\n",
    "        weekday=lambda x: x['EventDate'].dt.day_name(),\n",
    "        monthday=lambda x: x['EventDate'].dt.day,\n",
    "        is_holiday=lambda x: x['EventDate'].isin(df_mx_holidays['date']).astype(int),\n",
    "        is_release_day=lambda x: x['EventDate'].isin(df_releases['ESTRENO']).astype(int),\n",
    "        sin_weekday=lambda x: np.sin(2 * np.pi * x['EventDate'].dt.weekday / 7),\n",
    "        cos_weekday=lambda x: np.cos(2 * np.pi * x['EventDate'].dt.weekday / 7),\n",
    "        sin_monthday=lambda x: np.sin(2 * np.pi * x['EventDate'].dt.day / 30.4),\n",
    "        cos_monthday=lambda x: np.cos(2 * np.pi * x['EventDate'].dt.day / 30.4)\n",
    "    )\n",
    ")\n",
    "print(\"Daily panel with context:\", emails_daily_ctx.shape)\n",
    "\n",
    "# Preview context columns\n",
    "emails_daily_ctx[['EventDate','blockbuster_score','is_holiday','is_release_day','sin_weekday','cos_weekday']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b2b40",
   "metadata": {},
   "source": [
    "#### 3.6: Flag purchases\n",
    "- We join daily rows to same-day transactions to flag whether a purchase occurred (has_bought).\n",
    "- The head above shows purchase flags with select user features used later as controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553bf8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Step 6: Merge transactions to flag purchase\n",
    "trx_day = df_transactions.loc[:, ['CARD_MEMBERSHIPID', 'FECHA_TRANSACCION', 'has_children', 'total_visits', 'avg_spend_per_visit', 'favorite_cinema', 'transaction_id']]\n",
    "\n",
    "emails_daily_ctx_trx = (\n",
    "    emails_daily_ctx\n",
    "    .merge(trx_day, how='left', left_on=['CARD_MEMBERSHIPID', 'EventDate'], right_on=['CARD_MEMBERSHIPID', 'FECHA_TRANSACCION'])\n",
    "    .drop(columns=['FECHA_TRANSACCION'])\n",
    "    .assign(has_bought=lambda x: np.where(x['transaction_id'].notna(), 1, 0))\n",
    ")\n",
    "print(\"Panel after trx merge:\", emails_daily_ctx_trx.shape)\n",
    "print(\"Has_bought rate (%):\", round(emails_daily_ctx_trx['has_bought'].mean() * 100, 3))\n",
    "\n",
    "# Preview purchase labels and key user features\n",
    "emails_daily_ctx_trx[['EventDate','CARD_MEMBERSHIPID','has_bought','has_children','total_visits','favorite_cinema']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16432e",
   "metadata": {},
   "source": [
    "#### 3.7: Stabilize user attributes\n",
    "- We forward/backward fill stable user attributes so each daily row has consistent values.\n",
    "- The head above shows these stabilized features for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Step 7: Stabilize user features across the window (ffill/bfill)\n",
    "emails_daily_ctx_trx_fb = (\n",
    "    emails_daily_ctx_trx\n",
    "    .sort_values(by=['CARD_MEMBERSHIPID', 'EventDate'], ignore_index=True)\n",
    "    .assign(\n",
    "        has_children=lambda x: x.groupby('CARD_MEMBERSHIPID')['has_children'].ffill().bfill(),\n",
    "        avg_spend_per_visit=lambda x: x.groupby('CARD_MEMBERSHIPID')['avg_spend_per_visit'].ffill().bfill(),\n",
    "        favorite_cinema=lambda x: x.groupby('CARD_MEMBERSHIPID')['favorite_cinema'].ffill().bfill(),\n",
    "        total_visits=lambda x: x.groupby('CARD_MEMBERSHIPID')['total_visits'].ffill().bfill()\n",
    "    )\n",
    ")\n",
    "print(\"Panel after ffill/bfill:\", emails_daily_ctx_trx_fb.shape)\n",
    "\n",
    "# Preview stabilized features\n",
    "emails_daily_ctx_trx_fb[['CARD_MEMBERSHIPID','EventDate','has_children','total_visits','avg_spend_per_visit','favorite_cinema']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad58e79",
   "metadata": {},
   "source": [
    "#### 3.8: Final panel shape\n",
    "- Built a balanced pre/post window (K=7 days) around each email.\n",
    "- Enriched with calendar, holiday, release, and blockbuster context.\n",
    "- Labeled daily purchase outcomes to enable both ML and RDD estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.8 Step 8: Define RDD variables and finalize panel\n",
    "panel_final = emails_daily_ctx_trx_fb.assign(\n",
    "    is_post=lambda x: x['days_from_email'].gt(0).astype(int),\n",
    "    r=lambda x: np.where(x['days_from_email'].gt(0), x['days_from_email'], 0),\n",
    "    w=lambda x: 1 - x['days_from_email'].abs() / (K + 1),  # Triangular kernel weight\n",
    ")\n",
    "\n",
    "# Drop the exact email day (0) to avoid exposure mixing\n",
    "panel_final = panel_final.query(\"days_from_email != 0\").reset_index(drop=True)\n",
    "\n",
    "print(\"Final panel shape:\", panel_final.shape)\n",
    "print(\"Post-period share (%):\", round(panel_final['is_post'].mean() * 100, 3))\n",
    "print(\"Purchase rate (%):\", round(panel_final['has_bought'].mean() * 100, 3))\n",
    "\n",
    "# Preview finalized panel\n",
    "panel_final[['CARD_MEMBERSHIPID','EventDate','days_from_email','is_post','w','has_bought']].head()\n",
    "\n",
    "# Keep downstream variable name\n",
    "df_emails_expanded = panel_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f811b",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ML Model: Purchase Prediction and Uplift Read\n",
    "\n",
    "We train CatBoost to predict daily purchase probability in the panel and examine whether the post-email indicator is predictive after controlling for other drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b718e7",
   "metadata": {},
   "source": [
    "### 4.1: Define features/target and basic checks\n",
    "We prepare X, y, and sample weights (triangular kernel). We also verify missing values and basic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target for ML\n",
    "cols2use = [\n",
    "    'blockbuster_score',\n",
    "    'is_holiday',\n",
    "    'sin_weekday', 'cos_weekday',\n",
    "    'sin_monthday', 'cos_monthday',\n",
    "    'has_children', 'total_visits',\n",
    "    'favorite_cinema', 'SendId',\n",
    "    'is_post',  # treatment flag\n",
    "]\n",
    "cat_cols = ['is_post', 'is_holiday', 'has_children', 'favorite_cinema', 'SendId']\n",
    "\n",
    "X = df_emails_expanded[cols2use].copy()\n",
    "X['has_children'] = X['has_children'].fillna(-1).astype(int)\n",
    "X['is_post'] = X['is_post'].astype(int)\n",
    "X['is_holiday'] = X['is_holiday'].astype(int)\n",
    "X['favorite_cinema'] = X['favorite_cinema'].fillna(-1).astype(int)\n",
    "X['SendId'] = X['SendId'].astype(str)\n",
    "X[cat_cols] = X[cat_cols].astype('category')\n",
    "\n",
    "y = df_emails_expanded['has_bought'].copy()\n",
    "w_weights = df_emails_expanded['w'].copy()  # triangular kernel weights\n",
    "\n",
    "# Basic checks\n",
    "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
    "print(\"NaNs per column:\\n\", X.isna().sum())\n",
    "print(\"Class balance (bought=1) %:\", round(y.mean()*100,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38405cef",
   "metadata": {},
   "source": [
    "#### 4.2: Split, build pools, and train\n",
    "- Split into train/validation, stratifying by treatment (is_post) for balance.\n",
    "- Build CatBoost pools to handle categorical features and sample weights.\n",
    "- Train with early stopping using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train/validation split (stratify by treatment)\n",
    "X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "    X, y, w_weights, test_size=0.2, random_state=42, stratify=X['is_post']\n",
    ")\n",
    "print(f\"Train size: {X_train.shape}, Val size: {X_val.shape}\")\n",
    "\n",
    "# Step 3: Build CatBoost pools\n",
    "pool_train = Pool(X_train, y_train, weight=w_train, cat_features=cat_cols)\n",
    "pool_val = Pool(X_val, y_val, weight=w_val, cat_features=cat_cols)\n",
    "\n",
    "# Step 4: Train CatBoost\n",
    "model = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "model.fit(pool_train, eval_set=pool_val, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0196d3d",
   "metadata": {},
   "source": [
    "### 4.3: Evaluate and interpret\n",
    "- Report precision/recall/F1 for both classes.\n",
    "- Use SHAP to understand global importance and targeted relationships (including is_post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b005f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate classification metrics on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, target_names=['Not Bought', 'Bought'], digits=4))\n",
    "\n",
    "# Step 6: SHAP interpretation\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# Global importance summary\n",
    "shap.summary_plot(shap_values, X_val, plot_type=\"dot\", max_display=20)\n",
    "\n",
    "# Top features by mean |SHAP|\n",
    "shap_abs = np.abs(shap_values)\n",
    "feat_importance = pd.Series(shap_abs.mean(axis=0), index=X_val.columns).sort_values(ascending=False)\n",
    "print(\"Top features by mean |SHAP|:\\n\", feat_importance.head(10))\n",
    "\n",
    "# Targeted partials (including treatment)\n",
    "for feat in ['is_post', 'blockbuster_score', 'total_visits', 'is_holiday']:\n",
    "    shap.dependence_plot(feat, shap_values, X_val, interaction_index=None, show=False)\n",
    "    plt.title(f'Partial Dependence of {feat}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d0ac8",
   "metadata": {},
   "source": [
    "#### Insights: ML Model\n",
    "- The model captures calendar effects and user history while assessing the post-email flag.\n",
    "- Inspect SHAP for whether 'is_post' meaningfully shifts purchase probability after controlling for confounders.\n",
    "- Use caution: model-based association is not causal, but it helps detect patterns that RDD might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6620d",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. RDD: GLM with Cluster-Robust Errors\n",
    "\n",
    "We estimate the post-email jump using a simple logistic GLM, clustering standard errors by customer to account for within-user correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afd21e",
   "metadata": {},
   "source": [
    "### 5.1: Fit Linear Model with robust SEs\n",
    "We classify the day type and build a compact GLM formula with treatment and calendar controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Day type categorical: weekend, Wednesday, other (baseline)\n",
    "df_emails_expanded['type_of_day'] = np.where(\n",
    "    df_emails_expanded['EventDate'].dt.weekday.isin([5, 6]), 'weekend',\n",
    "    np.where(df_emails_expanded['EventDate'].dt.weekday == 2, 'wednesday', 'other')\n",
    ")\n",
    "print(df_emails_expanded['type_of_day'].value_counts(normalize=True).rename('%').mul(100).round(2))\n",
    "\n",
    "# Step 1b: Simple RDD formula: treatment + calendar controls\n",
    "formula = \"has_bought ~ is_post + C(type_of_day, Treatment('other')) + is_holiday\"\n",
    "print(\"RDD formula:\", formula)\n",
    "\n",
    "# Step 2: Ensure no missing values in used fields\n",
    "df_emails_expanded['has_children'] = df_emails_expanded['has_children'].fillna(0).astype(int)\n",
    "\n",
    "# Step 3: Fit GLM Binomial with cluster-robust SE at customer level\n",
    "rdd_model = smf.glm(formula, data=df_emails_expanded, family=sm.families.Binomial())\n",
    "glm_res = rdd_model.fit(cov_type='cluster', cov_kwds={'groups': df_emails_expanded['CARD_MEMBERSHIPID']})\n",
    "\n",
    "# Step 4: Report\n",
    "tbl = glm_res.summary()\n",
    "print(\"RD jump (is_post) on log-odds:\", glm_res.params['is_post'])\n",
    "print(\"RD jump (is_post) as odds ratio:\", np.exp(glm_res.params['is_post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look table\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da538c17",
   "metadata": {},
   "source": [
    "## 5.2 Plot effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Plot coefficients with 95% CIs\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': glm_res.params.index,\n",
    "    'coef': glm_res.params.values,\n",
    "    'conf_int_lower': glm_res.conf_int()[0],\n",
    "    'conf_int_upper': glm_res.conf_int()[1],\n",
    "    'p_value': glm_res.pvalues.values\n",
    "})\n",
    "coef_df = coef_df[coef_df['feature'] != 'Intercept']\n",
    "coef_df['feature'] = coef_df['feature'].str.replace(r\"C\\(type_of_day, Treatment\\('other'\\)\\)\\[T\\.\", '', regex=True)\n",
    "coef_df['feature'] = coef_df['feature'].str.replace(r\"\\]\", '', regex=True)\n",
    "coef_df['color'] = np.where(coef_df['p_value'] < 0.05, 'blue', 'red')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for _, row in coef_df.iterrows():\n",
    "    plt.errorbar(\n",
    "        row['feature'], row['coef'],\n",
    "        yerr=[[row['coef'] - row['conf_int_lower']], [row['conf_int_upper'] - row['coef']]],\n",
    "        fmt='o', color=row['color'], capsize=5, elinewidth=1, markeredgewidth=1\n",
    "    )\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('RDD Coefficients with 95% Confidence Intervals')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Predictive sanity check\n",
    "y_pred = glm_res.predict(df_emails_expanded)\n",
    "print(\"Share predicted >0.5:\", float((y_pred > 0.5).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332ad95",
   "metadata": {},
   "source": [
    "#### Insights: RDD GLM\n",
    "- The coefficient on 'is_post' measures the discontinuity in purchase probability right after emails, controlling for day-type and holidays, with customer-clustered SEs.\n",
    "- In this run, the estimate is small and statistically insignificant, indicating no clear uplift.\n",
    "- Caveats: local design (+/- K days), daily aggregation, and potential unobserved confounding (e.g., time-of-day or concurrent promos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e09f01",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion and Next Steps\n",
    "\n",
    "Summary\n",
    "- No statistically significant uplift detected in purchases immediately after emails in this specification.\n",
    "- ML reveals calendar and user-history as dominant drivers; the post-email signal is weak once controls are applied.\n",
    "\n",
    "Next steps\n",
    "- Increase granularity: incorporate send hour and within-day purchase timing.\n",
    "- Explore heterogeneous effects (e.g., by visit frequency, family presence, cinema region).\n",
    "- Bayesian RDD to quantify uncertainty more fully.\n",
    "- Design an RCT to establish causality and calibrate expected lift.\n",
    "- Consider geo experiments (treated vs. control cities) to reduce spillovers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
